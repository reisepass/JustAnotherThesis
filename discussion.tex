\chapter{Discussion}
While the EM Mitochondria dataset resulted in good performance of the pairwise model there were others like the MSRC dataset where the unary model was not significantly overtaken. As explained in the results section \ref{smoothness} we find that the pairwise models do produce much smoother (and therefore more natural) predictions. But this is not captured in the Hamming distance loss function and hence on some datasets the unary gets higher accuracy even if the prediction has single labels in a neighbourhood which we know a-priori would be impossible. We suggest for future work to develop a new loss function which evaluates the accuracy on the test set by also capturing the natural plausibility of the output produced. In many applications it is more important for the segmentations to be biologically plausible than having the highest per pixel match to the ground truth \todo{citation needed}. Some replacements to the Hamming distance have been purposed. The PASCAL natural image segmentation challenge was measured using the per class Jaccard distance \cite{everingham2010pascal}. A measure which encourages labelling with the correct size was pruposed by Pletscher and Kohli \cite{pletscher2012learning}. Balancing false positives versus false negatives with class priors was attempted by Lempitsky et. al. \cite{lempitsky2011pylon}. But all the above loss functions do not consider the topology of the labelled objects as a whole.
\par
Still with the simple weighted loss function used for all our training we achieve satisfying results with very good scalability and preliminary research has shown that the accuracy can be vastly improved by using more sophisticated features. 


\section{Expansion of Features}
We did not focus too much attention on the features as their choice is very dependent on the data at hand. Still for biological data we would like to expand the set of pre-installed features to include Ray projection features \ref{smith2009fast}. And recently deeplearning features have found a lot of success on natural image segmentation problems \cite{krizhevsky2012imagenet}. These Deep Learning features actually result in a much higher dimensional problem than what we are solving but since we solve it in the dual and representing alpha as sum of spares vectors the system is robust to the feature vector becoming very large. 
\section{Expansion of ScalaSLIC}
We chose to perform the preprocessing on single machine because it only needs to be done once before training and in order to keep this package simple as it will be published open-source separately. But in practice we found that the preprocessing takes a significant amount of time and could easily be distributed on a per image basis. Once this preprocessing step is distributed one could also expand the distance measure to include more complex features than just the LAB space distance. 
\section{Expansion of the SSVM}
One clear improvement we would have liked to test would be to transform the dual problem into a kernel space because it would allow us to find a non linear decision boundary. While it is clearly advantageous from a learning theory perspective it would come at additional computational complexity because to get $\weightVect$ for synchronization between nodes one would have to sum up all the support vectors chosen thus far. But it is worth attempting empirically because bringing the features into a space where classes are easily separable will make the max-oracle problem easier and hence one could possibly reduce the number of iterations used for LoopyBP or Mean Field. 
\section{Improving Automation}
As we described in section \ref{sec:Lambda} the lambda parameter is critical for good convergence of BCFW. The user is advises to always rerun their experiment with lambda constants at several scales to find the best range. Lambda could easily be selected automatically by minimizing the cross validation error on the training set. Additionally we explain the relevance of compactness parameter $M$ for the super-pixel preprocessing described in section \ref{slicParams}. The parameter $M$ is independent of the choice of lambda and could be optimized by running slick several times and optimizing for the most uniformity of ground truth labels within each super-pixel. 
